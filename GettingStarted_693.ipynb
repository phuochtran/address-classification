{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# NOTE: you CAN change this cell\n",
        "# If you want to use your own database, download it here\n",
        "!pip install -q gdown\n",
        "\n",
        "import gdown\n",
        "\n",
        "# province\n",
        "gdown.download(\"https://drive.google.com/uc?id=1TVG3CmtPnpPfFA8O_EMOgZnDQNzH9wxH\", \"list_province.txt\", quiet=False)\n",
        "\n",
        "# district\n",
        "gdown.download(\"https://drive.google.com/uc?id=16LnTv_Ruybrxw8MfbEaWB7GPrj7y4pzA\", \"list_district.txt\", quiet=False)\n",
        "\n",
        "# ward\n",
        "gdown.download(\"https://drive.google.com/uc?id=1Zn3FTe8OAMQqlOZsF1xcy2Jy2cQkRpyU\", \"list_ward.txt\", quiet=False)\n",
        "\n",
        "# database\n",
        "gdown.download(\"https://drive.google.com/uc?id=1aI19MkiUM2CZ9XbU7ERffo2uG--lBr0v\", \"list_full_hierarchy.txt\", quiet=False)"
      ],
      "metadata": {
        "id": "i20WfB6lqiUy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "73e47028-b2c2-43a4-89da-3034195cb5a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TVG3CmtPnpPfFA8O_EMOgZnDQNzH9wxH\n",
            "To: /content/list_province.txt\n",
            "100%|██████████| 749/749 [00:00<00:00, 2.60MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16LnTv_Ruybrxw8MfbEaWB7GPrj7y4pzA\n",
            "To: /content/list_district.txt\n",
            "100%|██████████| 7.75k/7.75k [00:00<00:00, 17.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Zn3FTe8OAMQqlOZsF1xcy2Jy2cQkRpyU\n",
            "To: /content/list_ward.txt\n",
            "100%|██████████| 88.3k/88.3k [00:00<00:00, 41.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aI19MkiUM2CZ9XbU7ERffo2uG--lBr0v\n",
            "To: /content/list_full_hierarchy.txt\n",
            "100%|██████████| 571k/571k [00:00<00:00, 102MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'list_full_hierarchy.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: you CAN change this cell\n",
        "# Add more to your needs\n",
        "# you must place ALL pip install here\n",
        "!pip install editdistance"
      ],
      "metadata": {
        "id": "J8znFuZTzwoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39299b55-68e7-44cf-9fac-f658cf3ea3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.12/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: you CAN change this cell\n",
        "# import your library here\n",
        "import re, csv, unicodedata, time\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "AodaIxYa32hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: you MUST change this cell\n",
        "# New methods / functions must be written under class Solution.\n",
        "# ================================================================\n",
        "class Solution:\n",
        "    # =========================\n",
        "    # ======= PUBLIC API ======\n",
        "    # =========================\n",
        "    def __init__(self):\n",
        "        # fixed file names (provided by the grader)\n",
        "        self.province_path  = 'list_province.txt'\n",
        "        self.district_path  = 'list_district.txt'\n",
        "        self.ward_path      = 'list_ward.txt'\n",
        "        self.hierarchy_path = 'list_full_hierarchy.txt'\n",
        "\n",
        "        # 1) load hierarchy (province -> district -> [wards])\n",
        "        self.address_hierarchy = self._build_address_hierarchy(self.hierarchy_path)\n",
        "\n",
        "        # 2) auto-generate abbreviations (no JSON needed)\n",
        "        self.auto_abbr = self._auto_generate_abbreviations(self.address_hierarchy)\n",
        "\n",
        "        # 3) load base lists for output normalization\n",
        "        self.provinces_raw = self._load_list(self.province_path)\n",
        "        self.districts_raw = self._load_list(self.district_path)\n",
        "        self.wards_raw     = self._load_list(self.ward_path)\n",
        "\n",
        "        # 4) OCR pairs\n",
        "        self._build_ocr_maps()\n",
        "\n",
        "        # 5) build context tries + norm→raw maps\n",
        "        (self.trie_prov, self.prov_norm2raw,\n",
        "         self.trie_dist_by_prov, self.dist_norm2raw_by_prov,\n",
        "         self.trie_ward_by_dist, self.ward_norm2raw_by_dist) = self._build_context_tries_v2(\n",
        "            self.address_hierarchy, self.auto_abbr\n",
        "        )\n",
        "\n",
        "    def process(self, s: str):\n",
        "        \"\"\"\n",
        "        Parse one free-form address string → {'province','district','ward'}\n",
        "        Pipeline:\n",
        "        - normalize + expand abbrev + OCR-fix\n",
        "        - detect province (trie)\n",
        "        - detect district theo province (trie)\n",
        "        - detect ward theo (province,district) (trie)\n",
        "        - fallback tìm theo substring trong hierarchy nếu thiếu\n",
        "        - guard bằng anchors \"phuong N\" để tránh chọn ward sai\n",
        "        - ưu tiên ward nằm TRƯỚC district trong chuỗi (thứ tự VN)\n",
        "        \"\"\"\n",
        "        texts = self._normalize_pipeline(s)\n",
        "        exp = texts[1]\n",
        "        anchors = self._extract_anchors(exp)\n",
        "\n",
        "        # 1) province\n",
        "        prov_cands = self._candidates_with_positions(texts, self.trie_prov, self.prov_norm2raw, level=\"prov\")\n",
        "        province = prov_cands[0][0] if prov_cands else \"\"\n",
        "        pkey = self._n_base(province) if province else \"\"\n",
        "\n",
        "        # 2) district by province (context)\n",
        "        district, dist_pos = \"\", None\n",
        "        if pkey and pkey in self.trie_dist_by_prov and pkey in self.dist_norm2raw_by_prov:\n",
        "            dist_cands = self._candidates_with_positions(texts, self.trie_dist_by_prov[pkey],\n",
        "                                                         self.dist_norm2raw_by_prov[pkey], level=\"dist\")\n",
        "            if dist_cands:\n",
        "                # Avoid pick district = province (TP Tuyên Quang vs Tỉnh Tuyên Quang) when label TP does not exist\n",
        "                best = dist_cands[0]\n",
        "                for c in dist_cands:\n",
        "                    if self._n_base(c[0]) != pkey:\n",
        "                        best = c\n",
        "                        break\n",
        "                district, _, st, en = best\n",
        "                dist_pos = (st, en)\n",
        "\n",
        "        dkey = self._n_base(district) if district else \"\"\n",
        "\n",
        "        # 3) ward theo (province,district) + prefer appear before district\n",
        "        ward = \"\"\n",
        "        if pkey and dkey and (pkey, dkey) in self.trie_ward_by_dist and (pkey, dkey) in self.ward_norm2raw_by_dist:\n",
        "            ward_cands = self._candidates_with_positions(texts,\n",
        "                                                         self.trie_ward_by_dist[(pkey, dkey)],\n",
        "                                                         self.ward_norm2raw_by_dist[(pkey, dkey)],\n",
        "                                                         level=\"ward\")\n",
        "            if dist_pos:\n",
        "                before = [c for c in ward_cands if c[3] < dist_pos[0]]  # c = (raw, score, st, en)\n",
        "                ward = before[0][0] if before else (ward_cands[0][0] if ward_cands else \"\")\n",
        "            elif ward_cands:\n",
        "                ward = ward_cands[0][0]\n",
        "\n",
        "            # Anchor-guard:\n",
        "            wn_set = set(self.ward_norm2raw_by_dist[(pkey, dkey)].values())\n",
        "            if anchors.get('ward_numbers'):\n",
        "                nums = [re.sub(r'^0+', '', x) for x in anchors['ward_numbers']]\n",
        "                if not any(n in wn_set for n in nums):\n",
        "                    if re.fullmatch(r'\\d{1,2}', self._n_base(ward)):\n",
        "                        ward = \"\"\n",
        "\n",
        "        # 4) fallback substring (missing p/d)\n",
        "        if not (province and district):\n",
        "            p2, d2, w2 = self._hier_lookup(' '.join(texts))\n",
        "            if not province: province = p2\n",
        "            if not district: district = d2\n",
        "            if not ward: ward = w2\n",
        "\n",
        "        return {\n",
        "            \"province\": province or \"\",\n",
        "            \"district\": district or \"\",\n",
        "            \"ward\":     ward or \"\"\n",
        "        }\n",
        "\n",
        "    # =========================\n",
        "    # ===== DATA BUILDERS =====\n",
        "    # =========================\n",
        "    def _load_list(self, path):\n",
        "        vals = []\n",
        "        try:\n",
        "            with open(path, encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    t = line.strip()\n",
        "                    if t: vals.append(t)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "        return vals\n",
        "\n",
        "    def _build_address_hierarchy(self, path):\n",
        "        \"\"\" Parse dia_chi_loc.txt → { province_raw : { district_raw : [ward_raw...] } } \"\"\"\n",
        "        hierarchy = defaultdict(lambda: defaultdict(list))\n",
        "        try:\n",
        "            with open(path, encoding='utf-8') as f:\n",
        "                reader = csv.reader(f)\n",
        "                for row in reader:\n",
        "                    if len(row) < 3:\n",
        "                        continue\n",
        "                    w, d, p = [x.strip() for x in row[:3]]\n",
        "                    hierarchy[p][d].append(w)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "        return hierarchy\n",
        "\n",
        "    def _auto_generate_abbreviations(self, hier):\n",
        "        \"\"\"Sinh alias đơn giản theo pattern + city aliases\"\"\"\n",
        "        abbr = {\"province\": {}, \"district\": {}, \"ward\": {}}\n",
        "        for p, dmap in hier.items():\n",
        "            abbr[\"province\"][p] = self._make_abbr(p)\n",
        "            for d, wards in dmap.items():\n",
        "                abbr[\"district\"][d] = self._make_abbr(d)\n",
        "                for w in wards:\n",
        "                    abbr[\"ward\"][w] = self._make_abbr(w)\n",
        "        # city aliases\n",
        "        abbr[\"province\"].setdefault(\"Thành phố Hồ Chí Minh\", []).extend([\"hcm\", \"tphcm\", \"tp hcm\"])\n",
        "        abbr[\"province\"].setdefault(\"Thành phố Hà Nội\", []).extend([\"hn\", \"tp ha noi\", \"tphn\"])\n",
        "        # dedup\n",
        "        for lvl in abbr:\n",
        "            for k, v in abbr[lvl].items():\n",
        "                abbr[lvl][k] = sorted(set(v))\n",
        "        return abbr\n",
        "\n",
        "    def _make_abbr(self, name):\n",
        "        n = self._n_base(self._strip_level_label(name))\n",
        "        out = []\n",
        "        # province-level\n",
        "        if re.match(r'^thanh pho\\b', n):\n",
        "            core = n.replace('thanh pho ', '')\n",
        "            out += [f'tp {core}', f'tp{core.replace(\" \",\"\")}']\n",
        "        if re.match(r'^tinh\\b', n):\n",
        "            core = n.replace('tinh ', '')\n",
        "            out += [f't {core}', f'tinh {core}']\n",
        "        # district-level\n",
        "        if re.match(r'^quan\\b', n):\n",
        "            core = n.replace('quan ', '')\n",
        "            out += [f'q {core}', f'q{core.replace(\" \",\"\")}', f'quan {core}']\n",
        "        if re.match(r'^huyen\\b', n):\n",
        "            core = n.replace('huyen ', '')\n",
        "            out += [f'h {core}', f'h{core.replace(\" \",\"\")}', f'huyen {core}']\n",
        "        if re.match(r'^thi xa\\b', n):\n",
        "            core = n.replace('thi xa ', '')\n",
        "            out += [f'tx {core}', f'tx{core.replace(\" \",\"\")}']\n",
        "        if re.match(r'^thi tran\\b', n):\n",
        "            core = n.replace('thi tran ', '')\n",
        "            out += [f'tt {core}', f'tt{core.replace(\" \",\"\")}']\n",
        "        # ward-level\n",
        "        if re.match(r'^phuong\\b', n):\n",
        "            core = n.replace('phuong ', '')\n",
        "            if re.fullmatch(r'\\d{1,2}', core):\n",
        "                out += [f'p {core}', f'p0{core}' if len(core)==1 else f'p{core}', f'phuong {core}']\n",
        "            else:\n",
        "                out += [f'p {core}', f'p{core.replace(\" \",\"\")}', f'phuong {core}']\n",
        "        if re.match(r'^xa\\b', n):\n",
        "            core = n.replace('xa ', '')\n",
        "            out += [f'x {core}', f'x{core.replace(\" \",\"\")}']\n",
        "        return out\n",
        "\n",
        "    # =========================\n",
        "    # ===== NORMALIZATION =====\n",
        "    # =========================\n",
        "    def _nfd_strip(self, s):\n",
        "        s = unicodedata.normalize(\"NFD\", s)\n",
        "        s = ''.join(ch for ch in s if unicodedata.category(ch) != 'Mn')\n",
        "        s = s.replace('đ','d').replace('Đ','D')\n",
        "        return unicodedata.normalize(\"NFC\", s)\n",
        "\n",
        "    def _n_base(self, s):\n",
        "        s = unicodedata.normalize(\"NFKC\", s)\n",
        "        s = self._nfd_strip(s).lower()\n",
        "        s = re.sub(r'[^a-z0-9]+', ' ', s)\n",
        "        s = re.sub(r'\\s+', ' ', s).strip()\n",
        "        return s\n",
        "\n",
        "    def _strip_level_label(self, s):\n",
        "        s0 = s.strip()\n",
        "        s0 = re.sub(r'^(Tỉnh|Thành\\s*phố)\\s+', '', s0, flags=re.I)\n",
        "        s0 = re.sub(r'^(Quận|Huyện|Thị\\s*xã|Thành\\s*phố)\\s+', '', s0, flags=re.I)\n",
        "        s0 = re.sub(r'^(Phường|Xã|Thị\\s*trấn)\\s+', '', s0, flags=re.I)\n",
        "        return s0.strip()\n",
        "\n",
        "    def _expand_text_abbrev(self, s_norm):\n",
        "        toks = s_norm.split()\n",
        "        rep = {'tp':'thanh pho','q':'quan','p':'phuong','tx':'thi xa','tt':'thi tran','h':'huyen','x':'xa'}\n",
        "        for i, t in enumerate(toks):\n",
        "            if t in rep: toks[i] = rep[t]\n",
        "        x = ' '.join(toks)\n",
        "        # q10 / p05 / h3...\n",
        "        x = re.sub(r'\\b(p|f)\\s*0?(\\d{1,2})\\b', r'phuong \\2', x)\n",
        "        x = re.sub(r'\\bq\\s*0?(\\d{1,2})\\b', r'quan \\1', x)\n",
        "        x = re.sub(r'\\bh\\s*0?(\\d{1,2})\\b', r'huyen \\1', x)\n",
        "        return x\n",
        "\n",
        "    # =========================\n",
        "    # ======== OCR FIX ========\n",
        "    # =========================\n",
        "    def _build_ocr_maps(self):\n",
        "        self.ocr_pairs = [('0','o'),('1','l'),('1','i'),('5','s'),('8','b')]\n",
        "\n",
        "    def _apply_ocr_lite(self, s_norm):\n",
        "        x = s_norm\n",
        "        for a, b in self.ocr_pairs:\n",
        "            x = re.sub(rf'(?<=\\b){a}(?=\\w)', b, x)\n",
        "        return x\n",
        "\n",
        "    # =========================\n",
        "    # ========= TRIE ==========\n",
        "    # =========================\n",
        "    class _TrieNode:\n",
        "        __slots__ = (\"ch\",\"end\")\n",
        "        def __init__(self):\n",
        "            self.ch, self.end = {}, False\n",
        "\n",
        "    def _build_trie(self, words):\n",
        "        root = self._TrieNode()\n",
        "        for w in words:\n",
        "            cur = root\n",
        "            for c in w:\n",
        "                cur = cur.ch.setdefault(c, self._TrieNode())\n",
        "            cur.end = True\n",
        "        return root\n",
        "\n",
        "    def _trie_scan_all(self, text, root):\n",
        "        out, n = [], len(text)\n",
        "        for i in range(n):\n",
        "            cur, j, last_end = root, i, -1\n",
        "            while j < n and text[j] in cur.ch:\n",
        "                cur = cur.ch[text[j]]\n",
        "                if cur.end:\n",
        "                    last_end = j\n",
        "                j += 1\n",
        "            if last_end >= i:\n",
        "                out.append((text[i:last_end+1], i, last_end))\n",
        "        return out\n",
        "\n",
        "    # =========================\n",
        "    # == CONTEXT TRIES BUILD ==\n",
        "    # =========================\n",
        "    def _build_context_tries_v2(self, hier, abbr):\n",
        "        # Province\n",
        "        prov_items = []\n",
        "        prov_norm2raw = {}\n",
        "        for p in hier.keys():\n",
        "            p_plain = self._strip_level_label(p)\n",
        "            forms = [ self._n_base(p_plain) ] + [ self._n_base(x) for x in abbr[\"province\"].get(p, []) ]\n",
        "            for f in forms:\n",
        "                prov_items.append(f)\n",
        "                prov_norm2raw[f] = p_plain\n",
        "        trie_prov = self._build_trie(prov_items)\n",
        "\n",
        "        # District per province (key by normalized province)\n",
        "        trie_dist, dist_norm2raw = {}, {}\n",
        "        for p, dmap in hier.items():\n",
        "            p_plain = self._strip_level_label(p)\n",
        "            pkey = self._n_base(p_plain)\n",
        "            items = []\n",
        "            for d in dmap.keys():\n",
        "                d_plain = self._strip_level_label(d)\n",
        "                d_out = re.sub(r'^0+', '', d_plain) if re.fullmatch(r'\\d{1,2}', self._n_base(d_plain)) else d_plain\n",
        "                forms = [ self._n_base(d_plain) ] + [ self._n_base(x) for x in abbr[\"district\"].get(d, []) ]\n",
        "                if re.fullmatch(r'\\d{1,2}', self._n_base(d_plain)):\n",
        "                    forms.append(self._n_base(re.sub(r'^0+', '', d_plain)))\n",
        "                for f in set(forms):\n",
        "                    items.append(f)\n",
        "                    dist_norm2raw.setdefault(pkey, {})[f] = d_out\n",
        "            trie_dist[pkey] = self._build_trie(items) if items else self._build_trie([])\n",
        "\n",
        "        # Ward per (province,district)\n",
        "        trie_ward, ward_norm2raw = {}, {}\n",
        "        for p, dmap in hier.items():\n",
        "            p_plain = self._strip_level_label(p)\n",
        "            pkey = self._n_base(p_plain)\n",
        "            for d, wards in dmap.items():\n",
        "                d_plain = self._strip_level_label(d)\n",
        "                dkey = self._n_base(d_plain)\n",
        "                items = []\n",
        "                for w in wards:\n",
        "                    w_plain = self._strip_level_label(w)\n",
        "                    w_out = re.sub(r'^0+', '', w_plain) if re.fullmatch(r'\\d{1,2}', self._n_base(w_plain)) else w_plain\n",
        "                    forms = [ self._n_base(w_plain) ] + [ self._n_base(x) for x in abbr[\"ward\"].get(w, []) ]\n",
        "                    if re.fullmatch(r'\\d{1,2}', self._n_base(w_plain)):\n",
        "                        forms.append(self._n_base(re.sub(r'^0+', '', w_plain)))\n",
        "                    for f in set(forms):\n",
        "                        items.append(f)\n",
        "                        ward_norm2raw.setdefault((pkey, dkey), {})[f] = w_out\n",
        "                trie_ward[(pkey, dkey)] = self._build_trie(items) if items else self._build_trie([])\n",
        "        return trie_prov, prov_norm2raw, trie_dist, dist_norm2raw, trie_ward, ward_norm2raw\n",
        "\n",
        "    # =========================\n",
        "    # ===== CANDIDATES + CTX ==\n",
        "    # =========================\n",
        "    def _normalize_pipeline(self, s):\n",
        "        base = self._n_base(s)\n",
        "        exp  = self._expand_text_abbrev(base)\n",
        "        ocr  = self._apply_ocr_lite(exp)\n",
        "        return [base, exp, ocr]\n",
        "\n",
        "    def _label_score(self, level, left, right, token_norm):\n",
        "        score = 0.0\n",
        "        if level == \"ward\":\n",
        "            if re.search(r'\\b(p|phuong|x|xa|tt|thi tran)\\b\\s*$', left): score += 0.25\n",
        "            if re.search(r'\\b(q|quan|h|huyen|tx|thi xa|tp|thanh pho)\\b\\s*$', left): score -= 0.15\n",
        "            if re.fullmatch(r'\\d{1,2}', token_norm) and not (re.search(r'\\b(p|phuong)\\b', left) or re.search(r'\\b(p|phuong)\\b', right)):\n",
        "                score -= 0.35\n",
        "        elif level == \"dist\":\n",
        "            if re.search(r'\\b(q|quan|h|huyen|tx|thi xa|tp|thanh pho)\\b\\s*$', left): score += 0.22\n",
        "            if re.search(r'\\b(p|phuong|x|xa|tt|thi tran)\\b\\s*$', left): score -= 0.20\n",
        "            if re.fullmatch(r'\\d{1,2}', token_norm) and not (re.search(r'\\b(q|quan|h|huyen)\\b', left) or re.search(r'\\b(q|quan|h|huyen)\\b', right)):\n",
        "                score -= 0.30\n",
        "        elif level == \"prov\":\n",
        "            if re.search(r'\\b(tinh|thanh pho|tp)\\b\\s*$', left): score += 0.12\n",
        "        return score\n",
        "\n",
        "    def _candidates_with_positions(self, texts, trie, norm2raw, level):\n",
        "        rows = []\n",
        "        for t in texts:\n",
        "            L = max(1, len(t))\n",
        "            for h, st, en in self._trie_scan_all(t, trie):\n",
        "                base_sc = 0.55 + len(h)/30.0 + 0.05*((en+1)/L)\n",
        "                left = t[max(0, st-16):st]\n",
        "                right = t[en+1:min(len(t), en+1+16)]\n",
        "                sc = base_sc + self._label_score(level, left, right, h)\n",
        "                if h in norm2raw:\n",
        "                    rows.append((norm2raw[h], sc, st, en))\n",
        "        rows.sort(key=lambda x: (-x[1], x[2]))  # score desc, then earlier position\n",
        "        return rows\n",
        "\n",
        "    def _extract_anchors(self, text_norm_expanded):\n",
        "        anchors = {}\n",
        "        anchors['ward_numbers'] = re.findall(r'\\bphuong\\s+0?(\\d{1,2})\\b', text_norm_expanded)\n",
        "        anchors['district_numbers'] = re.findall(r'\\b(quan|huyen)\\s+0?(\\d{1,2})\\b', text_norm_expanded)\n",
        "        return anchors\n",
        "\n",
        "    # =========================\n",
        "    # ====== FALLBACK LOOKUP ==\n",
        "    # =========================\n",
        "    def _hier_lookup(self, text):\n",
        "        txt = self._expand_text_abbrev(self._n_base(text)).replace(\" \", \"\")\n",
        "        for p, dmap in self.address_hierarchy.items():\n",
        "            p_plain = self._strip_level_label(p)\n",
        "            pnorm = self._n_base(p_plain).replace(\" \", \"\")\n",
        "            if pnorm in txt:\n",
        "                for d, wards in dmap.items():\n",
        "                    d_plain = self._strip_level_label(d)\n",
        "                    dnorm = self._n_base(d_plain).replace(\" \", \"\")\n",
        "                    if dnorm in txt:\n",
        "                        for w in wards:\n",
        "                            w_plain = self._strip_level_label(w)\n",
        "                            wnorm = self._n_base(w_plain).replace(\" \", \"\")\n",
        "                            if wnorm in txt:\n",
        "                                return p_plain, d_plain, w_plain\n",
        "                        return p_plain, d_plain, \"\"\n",
        "                return p_plain, \"\", \"\"\n",
        "        return \"\", \"\", \"\"\n"
      ],
      "metadata": {
        "id": "xtwG3tBDzMLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: DO NOT change this cell\n",
        "!rm -rf test.json\n",
        "# this link is public test\n",
        "!gdown --fuzzy https://drive.google.com/file/d/1PBt3U9I3EH885CDhcXspebyKI5Vw6uLB/view?usp=sharing -O test.json"
      ],
      "metadata": {
        "id": "7Sdb3ddTr1Jz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944aeb42-e734-433c-e7c1-29dcc4f02b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PBt3U9I3EH885CDhcXspebyKI5Vw6uLB\n",
            "To: /content/test.json\n",
            "\r  0% 0.00/79.4k [00:00<?, ?B/s]\r100% 79.4k/79.4k [00:00<00:00, 34.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CORRECT TESTS\n",
        "groups_province = {}\n",
        "groups_district = {'hòa bình': ['Hoà Bình', 'Hòa Bình'], 'kbang': ['Kbang', 'KBang'], 'quy nhơn': ['Qui Nhơn', 'Quy Nhơn']}\n",
        "groups_ward = {'ái nghĩa': ['ái Nghĩa', 'Ái Nghĩa'], 'ái quốc': ['ái Quốc', 'Ái Quốc'], 'ái thượng': ['ái Thượng', 'Ái Thượng'], 'ái tử': ['ái Tử', 'Ái Tử'], 'ấm hạ': ['ấm Hạ', 'Ấm Hạ'], 'an ấp': ['An ấp', 'An Ấp'], 'ẳng cang': ['ẳng Cang', 'Ẳng Cang'], 'ẳng nưa': ['ẳng Nưa', 'Ẳng Nưa'], 'ẳng tở': ['ẳng Tở', 'Ẳng Tở'], 'an hòa': ['An Hoà', 'An Hòa'], 'ayun': ['Ayun', 'AYun'], 'bắc ái': ['Bắc ái', 'Bắc Ái'], 'bảo ái': ['Bảo ái', 'Bảo Ái'], 'bình hòa': ['Bình Hoà', 'Bình Hòa'], 'châu ổ': ['Châu ổ', 'Châu Ổ'], 'chư á': ['Chư á', 'Chư Á'], 'chư rcăm': ['Chư Rcăm', 'Chư RCăm'], 'cộng hòa': ['Cộng Hoà', 'Cộng Hòa'], 'cò nòi': ['Cò  Nòi', 'Cò Nòi'], 'đại ân 2': ['Đại Ân  2', 'Đại Ân 2'], 'đak ơ': ['Đak ơ', 'Đak Ơ'], \"đạ m'ri\": [\"Đạ M'ri\", \"Đạ M'Ri\"], 'đông hòa': ['Đông Hoà', 'Đông Hòa'], 'đồng ích': ['Đồng ích', 'Đồng Ích'], 'hải châu i': ['Hải Châu  I', 'Hải Châu I'], 'hải hòa': ['Hải Hoà', 'Hải Hòa'], 'hành tín đông': ['Hành Tín  Đông', 'Hành Tín Đông'], 'hiệp hòa': ['Hiệp Hoà', 'Hiệp Hòa'], 'hòa bắc': ['Hoà Bắc', 'Hòa Bắc'], 'hòa bình': ['Hoà Bình', 'Hòa Bình'], 'hòa châu': ['Hoà Châu', 'Hòa Châu'], 'hòa hải': ['Hoà Hải', 'Hòa Hải'], 'hòa hiệp trung': ['Hoà Hiệp Trung', 'Hòa Hiệp Trung'], 'hòa liên': ['Hoà Liên', 'Hòa Liên'], 'hòa lộc': ['Hoà Lộc', 'Hòa Lộc'], 'hòa lợi': ['Hoà Lợi', 'Hòa Lợi'], 'hòa long': ['Hoà Long', 'Hòa Long'], 'hòa mạc': ['Hoà Mạc', 'Hòa Mạc'], 'hòa minh': ['Hoà Minh', 'Hòa Minh'], 'hòa mỹ': ['Hoà Mỹ', 'Hòa Mỹ'], 'hòa phát': ['Hoà Phát', 'Hòa Phát'], 'hòa phong': ['Hoà Phong', 'Hòa Phong'], 'hòa phú': ['Hoà Phú', 'Hòa Phú'], 'hòa phước': ['Hoà Phước', 'Hòa Phước'], 'hòa sơn': ['Hoà Sơn', 'Hòa Sơn'], 'hòa tân': ['Hoà Tân', 'Hòa Tân'], 'hòa thuận': ['Hoà Thuận', 'Hòa Thuận'], 'hòa tiến': ['Hoà Tiến', 'Hòa Tiến'], 'hòa trạch': ['Hoà Trạch', 'Hòa Trạch'], 'hòa vinh': ['Hoà Vinh', 'Hòa Vinh'], 'hương hòa': ['Hương Hoà', 'Hương Hòa'], 'ích hậu': ['ích Hậu', 'Ích Hậu'], 'ít ong': ['ít Ong', 'Ít Ong'], 'khánh hòa': ['Khánh Hoà', 'Khánh Hòa'], 'krông á': ['Krông Á', 'KRông á'], 'lộc hòa': ['Lộc Hoà', 'Lộc Hòa'], 'minh hòa': ['Minh Hoà', 'Minh Hòa'], 'mường ải': ['Mường ải', 'Mường Ải'], 'mường ẳng': ['Mường ẳng', 'Mường Ẳng'], 'nậm ét': ['Nậm ét', 'Nậm Ét'], 'nam hòa': ['Nam Hoà', 'Nam Hòa'], 'na ư': ['Na ư', 'Na Ư'], 'ngã sáu': ['Ngã sáu', 'Ngã Sáu'], 'nghi hòa': ['Nghi Hoà', 'Nghi Hòa'], 'nguyễn úy': ['Nguyễn Uý', 'Nguyễn úy', 'Nguyễn Úy'], 'nhân hòa': ['Nhân Hoà', 'Nhân Hòa'], 'nhơn hòa': ['Nhơn Hoà', 'Nhơn Hòa'], 'nhơn nghĩa a': ['Nhơn nghĩa A', 'Nhơn Nghĩa A'], 'phúc ứng': ['Phúc ứng', 'Phúc Ứng'], 'phước hòa': ['Phước Hoà', 'Phước Hòa'], 'sơn hóa': ['Sơn Hoá', 'Sơn Hóa'], 'tạ an khương đông': ['Tạ An Khương  Đông', 'Tạ An Khương Đông'], 'tạ an khương nam': ['Tạ An Khương  Nam', 'Tạ An Khương Nam'], 'tăng hòa': ['Tăng Hoà', 'Tăng Hòa'], 'tân hòa': ['Tân Hoà', 'Tân Hòa'], 'tân hòa thành': ['Tân Hòa  Thành', 'Tân Hòa Thành'], 'tân khánh trung': ['Tân  Khánh Trung', 'Tân Khánh Trung'], 'tân lợi': ['Tân lợi', 'Tân Lợi'], 'thái hòa': ['Thái Hoà', 'Thái Hòa'], 'thiết ống': ['Thiết ống', 'Thiết Ống'], 'thuận hòa': ['Thuận Hoà', 'Thuận Hòa'], 'thượng ấm': ['Thượng ấm', 'Thượng Ấm'], 'thụy hương': ['Thuỵ Hương', 'Thụy Hương'], 'thủy xuân': ['Thuỷ Xuân', 'Thủy Xuân'], 'tịnh ấn đông': ['Tịnh ấn Đông', 'Tịnh Ấn Đông'], 'tịnh ấn tây': ['Tịnh ấn Tây', 'Tịnh Ấn Tây'], 'triệu ái': ['Triệu ái', 'Triệu Ái'], 'triệu ẩu': ['Triệu ẩu', 'Triệu Ẩu'], 'trung hòa': ['Trung Hoà', 'Trung Hòa'], 'trung ý': ['Trung ý', 'Trung Ý'], 'tùng ảnh': ['Tùng ảnh', 'Tùng Ảnh'], 'úc kỳ': ['úc Kỳ', 'Úc Kỳ'], 'ứng hòe': ['ứng Hoè', 'Ứng Hoè'], 'vĩnh hòa': ['Vĩnh Hoà', 'Vĩnh Hòa'], 'vũ hòa': ['Vũ Hoà', 'Vũ Hòa'], 'xuân ái': ['Xuân ái', 'Xuân Ái'], 'xuân áng': ['Xuân áng', 'Xuân Áng'], 'xuân hòa': ['Xuân Hoà', 'Xuân Hòa'], 'xuất hóa': ['Xuất Hoá', 'Xuất Hóa'], 'ỷ la': ['ỷ La', 'Ỷ La']}\n",
        "groups_ward.update({1: ['1', '01'], 2: ['2', '02'], 3: ['3', '03'], 4: ['4', '04'], 5: ['5', '05'], 6: ['6', '06'], 7: ['7', '07'], 8: ['8', '08'], 9: ['9', '09']})\n",
        "def to_same(groups):\n",
        "    same = {ele: k for k, v in groups.items() for ele in v}\n",
        "    return same\n",
        "same_province = to_same(groups_province)\n",
        "same_district = to_same(groups_district)\n",
        "same_ward = to_same(groups_ward)\n",
        "def normalize(text, same_dict):\n",
        "    return same_dict.get(text, text)"
      ],
      "metadata": {
        "id": "3KN8RZL6tFzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEAM_NAME = 'HK251'\n",
        "EXCEL_FILE = f'{TEAM_NAME}.xlsx'\n",
        "\n",
        "import json\n",
        "import time\n",
        "with open('test.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "summary_only = True\n",
        "df = []\n",
        "solution = Solution()\n",
        "timer = []\n",
        "correct = 0\n",
        "for test_idx, data_point in enumerate(data):\n",
        "    address = data_point[\"text\"]\n",
        "\n",
        "    ok = 0\n",
        "    try:\n",
        "        answer = data_point[\"result\"]\n",
        "        answer[\"province_normalized\"] = normalize(answer[\"province\"], same_province)\n",
        "        answer[\"district_normalized\"] = normalize(answer[\"district\"], same_district)\n",
        "        answer[\"ward_normalized\"] = normalize(answer[\"ward\"], same_ward)\n",
        "\n",
        "        start = time.perf_counter_ns()\n",
        "        result = solution.process(address)\n",
        "        finish = time.perf_counter_ns()\n",
        "        timer.append(finish - start)\n",
        "        result[\"province_normalized\"] = normalize(result[\"province\"], same_province)\n",
        "        result[\"district_normalized\"] = normalize(result[\"district\"], same_district)\n",
        "        result[\"ward_normalized\"] = normalize(result[\"ward\"], same_ward)\n",
        "\n",
        "        province_correct = int(answer[\"province_normalized\"] == result[\"province_normalized\"])\n",
        "        district_correct = int(answer[\"district_normalized\"] == result[\"district_normalized\"])\n",
        "        ward_correct = int(answer[\"ward_normalized\"] == result[\"ward_normalized\"])\n",
        "        ok = province_correct + district_correct + ward_correct\n",
        "\n",
        "        df.append([\n",
        "            test_idx,\n",
        "            address,\n",
        "            answer[\"province\"],\n",
        "            result[\"province\"],\n",
        "            answer[\"province_normalized\"],\n",
        "            result[\"province_normalized\"],\n",
        "            province_correct,\n",
        "            answer[\"district\"],\n",
        "            result[\"district\"],\n",
        "            answer[\"district_normalized\"],\n",
        "            result[\"district_normalized\"],\n",
        "            district_correct,\n",
        "            answer[\"ward\"],\n",
        "            result[\"ward\"],\n",
        "            answer[\"ward_normalized\"],\n",
        "            result[\"ward_normalized\"],\n",
        "            ward_correct,\n",
        "            ok,\n",
        "            timer[-1] / 1_000_000_000,\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        print(f\"{answer = }\")\n",
        "        print(f\"{result = }\")\n",
        "        df.append([\n",
        "            test_idx,\n",
        "            address,\n",
        "            answer[\"province\"],\n",
        "            \"EXCEPTION\",\n",
        "            answer[\"province_normalized\"],\n",
        "            \"EXCEPTION\",\n",
        "            0,\n",
        "            answer[\"district\"],\n",
        "            \"EXCEPTION\",\n",
        "            answer[\"district_normalized\"],\n",
        "            \"EXCEPTION\",\n",
        "            0,\n",
        "            answer[\"ward\"],\n",
        "            \"EXCEPTION\",\n",
        "            answer[\"ward_normalized\"],\n",
        "            \"EXCEPTION\",\n",
        "            0,\n",
        "            0,\n",
        "            0,\n",
        "        ])\n",
        "        # any failure count as a zero correct\n",
        "        pass\n",
        "    correct += ok\n",
        "\n",
        "\n",
        "    if not summary_only:\n",
        "        # responsive stuff\n",
        "        print(f\"Test {test_idx:5d}/{len(data):5d}\")\n",
        "        print(f\"Correct: {ok}/3\")\n",
        "        print(f\"Time Executed: {timer[-1] / 1_000_000_000:.4f}\")\n",
        "\n",
        "\n",
        "print(f\"-\"*30)\n",
        "total = len(data) * 3\n",
        "score_scale_10 = round(correct / total * 10, 2)\n",
        "if len(timer) == 0:\n",
        "    timer = [0]\n",
        "max_time_sec = round(max(timer) / 1_000_000_000, 4)\n",
        "avg_time_sec = round((sum(timer) / len(timer)) / 1_000_000_000, 4)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df2 = pd.DataFrame(\n",
        "    [[correct, total, score_scale_10, max_time_sec, avg_time_sec]],\n",
        "    columns=['correct', 'total', 'score / 10', 'max_time_sec', 'avg_time_sec',],\n",
        ")\n",
        "\n",
        "columns = [\n",
        "    'ID',\n",
        "    'text',\n",
        "    'province',\n",
        "    'province_student',\n",
        "    'province_normalized',\n",
        "    'province_student_normalized',\n",
        "    'province_correct',\n",
        "    'district',\n",
        "    'district_student',\n",
        "    'district_normalized',\n",
        "    'district_student_normalized',\n",
        "    'district_correct',\n",
        "    'ward',\n",
        "    'ward_student',\n",
        "    'ward_normalized',\n",
        "    'ward_student_normalized',\n",
        "    'ward_correct',\n",
        "    'total_correct',\n",
        "    'time_sec',\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(df)\n",
        "df.columns = columns\n",
        "\n",
        "print(f'{TEAM_NAME = }')\n",
        "print(f'{EXCEL_FILE = }')\n",
        "print(df2)\n",
        "\n",
        "!pip install xlsxwriter\n",
        "writer = pd.ExcelWriter(EXCEL_FILE, engine='xlsxwriter')\n",
        "df2.to_excel(writer, index=False, sheet_name='summary')\n",
        "df.to_excel(writer, index=False, sheet_name='details')\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjO6FFcA0DYi",
        "outputId": "8876e73e-7779-4d40-bbcb-78fb541e19fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "TEAM_NAME = 'HK251'\n",
            "EXCEL_FILE = 'HK251.xlsx'\n",
            "   correct  total  score / 10  max_time_sec  avg_time_sec\n",
            "0      936   1350        6.93        0.0042        0.0005\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.12/dist-packages (3.2.9)\n"
          ]
        }
      ]
    }
  ]
}